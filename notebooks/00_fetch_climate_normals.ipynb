{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7238948-8c69-4ff6-a14c-ff5e7b5654f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and validate hourly and daily normals data from NCEI. Exclude stations that don't have data we need. Export data and a list of validated stations for later use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61e8a6b3-b823-46dd-a271-8d62c7434130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python tools and Jupyter config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27cdbea4-067e-40fe-8a0e-52d79145a835",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import requests\n",
    "import pandas as pd\n",
    "import jupyter_black\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fdd1fb86-bfab-456e-84d7-de59845d9393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the absolute paths for input and output files\n",
    "BASE = Path.cwd()\n",
    "STATIONS_HOURLY = BASE / \"../data/reference/socal_stations_hourly.json\"\n",
    "STATIONS_DAILY = BASE / \"../data/reference/socal_stations_daily.json\"\n",
    "DAILY_JSON_OUT = BASE / \"../data/processed/daily_normals.json\"\n",
    "HOURLY_JSON_OUT = BASE / \"../data/processed/hourly_normals.json\"\n",
    "DAILY_CSV_OUT = BASE / \"../data/processed/daily_normals.csv\"\n",
    "HOURLY_CSV_OUT = BASE / \"../data/processed/hourly_normals.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b20fb67c-a7db-4a98-90b8-5672c6474778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base URLs for hourly and daily data\n",
    "base_urls = {\n",
    "    \"hourly\": \"https://www.ncei.noaa.gov/data/normals-hourly/2006-2020/access/{}.csv\",\n",
    "    \"daily\": \"https://www.ncei.noaa.gov/data/normals-daily/2006-2020/access/{}.csv\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e442fb6-ec69-4140-a4c1-60b3a1b6725c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to retain\n",
    "hourly_columns = [\n",
    "    \"STATION\", \"NAME\", \"LATITUDE\", \"LONGITUDE\", \"ELEVATION\", \"DATE\", \"month\", \"day\", \"hour\",\n",
    "    \"HLY-TEMP-NORMAL\", \"HLY-DEWP-NORMAL\", \"HLY-PRES-NORMAL\", \"HLY-CLOD-PCTOVC\",\n",
    "    \"HLY-WIND-AVGSPD\", \"HLY-WIND-VCTDIR\"\n",
    "]\n",
    "\n",
    "daily_columns = [\n",
    "    \"STATION\", \"DATE\", \"LATITUDE\", \"LONGITUDE\", \"ELEVATION\", \"NAME\", \"month\", \"day\",\n",
    "    \"DLY-TAVG-NORMAL\", \"DLY-TMAX-NORMAL\", \"DLY-TMIN-NORMAL\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "400c0f97-6351-49fd-88fe-f9fffe886369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(stations_file, frequency, columns):\n",
    "    with open(stations_file, \"r\") as f:\n",
    "        stations = json.load(f)\n",
    "\n",
    "    data_frames = []\n",
    "    valid_stations = []\n",
    "    for station_id in stations.keys():\n",
    "        url = base_urls[frequency].format(station_id)\n",
    "        data = None\n",
    "        # Try different delimiters and quote settings\n",
    "        for delimiter in [',', '\\t']:\n",
    "            for quoting in [0, 1, 2, 3]:\n",
    "                try:\n",
    "                    data = pd.read_csv(url, delimiter=delimiter, quoting=quoting)\n",
    "                    if all(column in data.columns for column in columns):\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    data = None\n",
    "            if data is not None and all(column in data.columns for column in columns):\n",
    "                break\n",
    "        \n",
    "        if data is not None and all(column in data.columns for column in columns):\n",
    "            data = data[columns]\n",
    "            data_frames.append(data)\n",
    "            valid_stations.append(station_id)\n",
    "            # print(f\"Loaded data for station {station_id} ({frequency})\")\n",
    "        else:\n",
    "            print(f\"Station {station_id} ({frequency}) doesn't have enough data. Excluded.\")\n",
    "\n",
    "    if data_frames:\n",
    "        return pd.concat(data_frames, ignore_index=True), valid_stations\n",
    "    else:\n",
    "        return pd.DataFrame(columns=columns), valid_stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac1c8902-9407-4fdf-9a81-9ded34936d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Station USC00040144 (daily) doesn't have enough data. Excluded.\n",
      "Station US1CARV0008 (daily) doesn't have enough data. Excluded.\n",
      "Station US1CALA0001 (daily) doesn't have enough data. Excluded.\n",
      "Station US1CAOR0019 (daily) doesn't have enough data. Excluded.\n",
      "Station USC00046663 (daily) doesn't have enough data. Excluded.\n",
      "Station USC00047306 (daily) doesn't have enough data. Excluded.\n",
      "Station USC00047776 (daily) doesn't have enough data. Excluded.\n",
      "Station US1CALA0010 (daily) doesn't have enough data. Excluded.\n"
     ]
    }
   ],
   "source": [
    "# Load hourly data\n",
    "hourly_data, valid_hourly_stations = load_data(STATIONS_HOURLY, \"hourly\", hourly_columns)\n",
    "\n",
    "# Load daily data\n",
    "daily_data, valid_daily_stations = load_data(STATIONS_DAILY, \"daily\", daily_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79cad472-5b9d-4962-8ffc-360d5acb864b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aafdc674-7954-446b-8f5c-4b401b011933",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_data.columns = hourly_data.columns.str.lower().str.replace('hly-', '').str.replace('-', '_')\n",
    "daily_data.columns = daily_data.columns.str.lower().str.replace('dly-', '').str.replace('-', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f777c59-83fd-4c46-b5f9-4bf3d4404be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_df = hourly_data[['station', 'name', 'date',\n",
    "       'month', 'day', 'hour', 'temp_normal', 'dewp_normal', 'pres_normal',\n",
    "       'clod_pctovc', 'wind_avgspd', 'wind_vctdir', 'latitude', 'longitude', 'elevation']].rename(columns={'clod_pctovc':'pct_overcast', 'wind_avgspd':'avg_windspeed', 'wind_vctdir':'wind_direction', 'dewp_normal':'dewpoint_normal', 'pres_normal':'precip_normal'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94a2377e-b81b-4476-b20d-8662166979f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_df = daily_data[['station', 'name', 'date', \n",
    "       'month', 'day', 'tavg_normal', 'tmax_normal', 'tmin_normal', 'latitude', 'longitude']].rename(columns={'tavg_normal':'avg_temp_normal', 'tmax_normal':'max_temp_normal', 'tmin_normal':'min_temp_normal'}).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07c63bd3-239b-4375-9870-a697bdf5f728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the data to JSON for further processing if needed\n",
    "hourly_df.to_json(HOURLY_JSON_OUT, orient='records', indent=4)\n",
    "daily_df.to_json(DAILY_JSON_OUT, orient='records', indent=4)\n",
    "hourly_df.to_json(HOURLY_CSV_OUT, index=False)\n",
    "daily_df.to_json(DAILY_CSV_OUT, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3621eb25-e5b7-40ca-badb-a5c9908a8756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total valid hourly stations: 13\n",
      "Total valid daily stations: 24\n"
     ]
    }
   ],
   "source": [
    "# Print the number of valid stations for debugging purposes\n",
    "print(f\"\\nTotal valid hourly stations: {len(valid_hourly_stations)}\")\n",
    "print(f\"Total valid daily stations: {len(valid_daily_stations)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab5fe18b-c456-4e34-8a40-758f18fa7f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the list of valid stations to new JSON files for reference\n",
    "valid_hourly_stations_file = BASE / \"../data/reference/valid_hourly_stations.json\"\n",
    "valid_daily_stations_file = BASE / \"../data/reference/valid_daily_stations.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e84c7842-cf94-4f80-a0d6-5794452db82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(valid_hourly_stations_file, \"w\") as f:\n",
    "    json.dump(valid_hourly_stations, f, indent=4)\n",
    "\n",
    "with open(valid_daily_stations_file, \"w\") as f:\n",
    "    json.dump(valid_daily_stations, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034fe117-9644-4e56-8c57-8721a263fe95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths for S3 storage\n",
    "S3_BUCKET = \"stilesdata.com\"\n",
    "S3_DAILY_JSON_KEY = f\"weather/normals/daily_normals_socal.json\"\n",
    "S3_HOURLY_JSON_KEY = f\"weather/normals/hourly_normals_socal.json\"\n",
    "S3_DAILY_CSV_KEY = f\"weather/normals/daily_normals_socal.csv\"\n",
    "S3_HOURLY_CSV_KEY = f\"weather/normals/hourly_normals_socal.csv\"\n",
    "\n",
    "# Initialize boto3 client with environment variables\n",
    "s3_client = boto3.client(\n",
    "    \"s3\",\n",
    "    aws_access_key_id=os.getenv(\"MY_AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"MY_AWS_SECRET_ACCESS_KEY\"),\n",
    "    aws_session_token=os.getenv(\"MY_AWS_SESSION_TOKEN\"),\n",
    ")\n",
    "\n",
    "# Upload the daily JSON file\n",
    "s3_client.upload_file(str(DAILY_JSON_OUT), S3_BUCKET, S3_DAILY_JSON_KEY)\n",
    "print(f\"JSON file uploaded to s3://{S3_BUCKET}/{S3_DAILY_JSON_KEY}\")\n",
    "\n",
    "# Upload the hourly JSON file\n",
    "s3_client.upload_file(str(HOURLY_JSON_OUT), S3_BUCKET, S3_HOURLY_JSON_KEY)\n",
    "print(f\"JSON file uploaded to s3://{S3_BUCKET}/{S3_HOURLY_JSON_KEY}\")\n",
    "\n",
    "# Upload the daily CSV file\n",
    "s3_client.upload_file(str(DAILY_CSV_OUT), S3_BUCKET, S3_DAILY_CSV_KEY)\n",
    "print(f\"JSON file uploaded to s3://{S3_BUCKET}/{S3_DAILY_CSV_KEY}\")\n",
    "\n",
    "# Upload the hourly CSV file\n",
    "s3_client.upload_file(str(HOURLY_CSV_OUT), S3_BUCKET, S3_HOURLY_CSV_KEY)\n",
    "print(f\"JSON file uploaded to s3://{S3_BUCKET}/{S3_HOURLY_CSV_KEY}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ff329cb1-4ef4-473f-9d43-f25b51b6121c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook 00_fetch_climate_normals.ipynb to script\n",
      "[NbConvertApp] Writing 6264 bytes to ../scripts/fetch_climate_normals.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script --no-prompt --output ../scripts/fetch_climate_normals 00_fetch_climate_normals.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067e8ca8-02cb-45aa-99ff-e70f098efb61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
